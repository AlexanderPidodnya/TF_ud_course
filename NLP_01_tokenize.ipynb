{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'My favorite food is ice cream',\n",
    "    'do you like ice cream too?',\n",
    "    'My dog likes ice cream!',\n",
    "    \"your favorite flavor of icecream is chocolate\",\n",
    "    \"chocolate isn't good for dogs\",\n",
    "    \"your dog, your cat, and your parrot prefer broccoli\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally set the max number of words to tokenize.\n",
    "# The out of vocabulary (OOV) token represents words that are not in the index.\n",
    "# Call fit_on_text() on the tokenizer to generate unique numbers for each word\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'your': 2, 'ice': 3, 'cream': 4, 'my': 5, 'favorite': 6, 'is': 7, 'dog': 8, 'chocolate': 9, 'food': 10, 'do': 11, 'you': 12, 'like': 13, 'too': 14, 'likes': 15, 'flavor': 16, 'of': 17, 'icecream': 18, \"isn't\": 19, 'good': 20, 'for': 21, 'dogs': 22, 'cat': 23, 'and': 24, 'parrot': 25, 'prefer': 26, 'broccoli': 27}\n"
     ]
    }
   ],
   "source": [
    "# Examine the word index\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Get the number for a given word\n",
    "print(word_index['favorite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 10, 7, 3, 4], [11, 12, 13, 3, 4, 14], [5, 8, 15, 3, 4], [2, 6, 16, 17, 18, 7, 9], [9, 19, 20, 21, 22], [2, 8, 2, 23, 24, 2, 25, 26, 27]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print (sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'your': 2, 'ice': 3, 'cream': 4, 'my': 5, 'favorite': 6, 'is': 7, 'dog': 8, 'chocolate': 9, 'food': 10, 'do': 11, 'you': 12, 'like': 13, 'too': 14, 'likes': 15, 'flavor': 16, 'of': 17, 'icecream': 18, \"isn't\": 19, 'good': 20, 'for': 21, 'dogs': 22, 'cat': 23, 'and': 24, 'parrot': 25, 'prefer': 26, 'broccoli': 27}\n",
      "\n",
      "Sequences =  [[5, 6, 10, 7, 3, 4], [11, 12, 13, 3, 4, 14], [5, 8, 15, 3, 4], [2, 6, 16, 17, 18, 7, 9], [9, 19, 20, 21, 22], [2, 8, 2, 23, 24, 2, 25, 26, 27]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  0  0  5  6 10  7  3  4]\n",
      " [ 0  0  0 11 12 13  3  4 14]\n",
      " [ 0  0  0  0  5  8 15  3  4]\n",
      " [ 0  0  2  6 16 17 18  7  9]\n",
      " [ 0  0  0  0  9 19 20 21 22]\n",
      " [ 2  8  2 23 24  2 25 26 27]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print('\\nWord Index = ', word_index)\n",
    "print('\\nSequences = ', sequences)\n",
    "print('\\nPadded Sequences:')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequences:\n",
      "[[ 5  6 10  7  3  4  0  0  0  0  0  0  0  0  0]\n",
      " [11 12 13  3  4 14  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  8 15  3  4  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  6 16 17 18  7  9  0  0  0  0  0  0  0  0]\n",
      " [ 9 19 20 21 22  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  8  2 23 24  2 25 26 27  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, maxlen=15, padding='post')\n",
    "\n",
    "print('\\nPadded Sequences:')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequences:\n",
      "[[ 7  3  4]\n",
      " [ 3  4 14]\n",
      " [15  3  4]\n",
      " [18  7  9]\n",
      " [20 21 22]\n",
      " [25 26 27]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, maxlen=3, padding='post')\n",
    "\n",
    "print('\\nPadded Sequences:')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"my best friend's favorite ice cream flavor is strawberry\", \"my dog's best friend is a manatee\"]\n",
      "<OOV> has the number 1 in the word index.\n",
      "\n",
      "Test Sequence =  [[5, 1, 1, 6, 3, 4, 16, 7, 1], [5, 1, 1, 1, 7, 1, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[ 0  5  1  1  6  3  4 16  7  1]\n",
      " [ 0  0  0  5  1  1  1  7  1  1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"my best friend's favorite ice cream flavor is strawberry\",\n",
    "    \"my dog's best friend is a manatee\"\n",
    "]\n",
    "print (test_data)\n",
    "\n",
    "# Remind ourselves which number corresponds to the\n",
    "# out of vocabulary token in the word index\n",
    "print(\"<OOV> has the number\", word_index['<OOV>'], \"in the word index.\")\n",
    "\n",
    "# Convert the test sentences to sequences\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "# Pad the new sequences\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "\n",
    "# Notice that \"1\" appears in the sequence wherever there's a word \n",
    "# that's not in the word index\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P\n",
      "127831/127831 [==============================] - 0s 0us/step\n",
      "C:\\Users\\Администратор.SHTAUF\\.keras\\datasets\\reviews.csv\n"
     ]
    }
   ],
   "source": [
    "path = tf.keras.utils.get_file('reviews.csv', 'https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P')\n",
    "print (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1987</td>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1988</td>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1989</td>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1990</td>\n",
       "      <td>The whole experience was underwhelming and I t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1991</td>\n",
       "      <td>Then as if I hadn't wasted enough of my life t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1992 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  sentiment\n",
       "0              0  So there is no way for me to plug it in here i...          0\n",
       "1              1                         Good case Excellent value.          1\n",
       "2              2                             Great for the jawbone.          1\n",
       "3              3  Tied to charger for conversations lasting more...          0\n",
       "4              4                                  The mic is great.          1\n",
       "...          ...                                                ...        ...\n",
       "1987        1987  I think food should have flavor and texture an...          0\n",
       "1988        1988                           Appetite instantly gone.          0\n",
       "1989        1989  Overall I was not impressed and would not go b...          0\n",
       "1990        1990  The whole experience was underwhelming and I t...          0\n",
       "1991        1991  Then as if I hadn't wasted enough of my life t...          0\n",
       "\n",
       "[1992 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(path, sep=',')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " 'Good case Excellent value.',\n",
       " 'Great for the jawbone.',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       " 'The mic is great.',\n",
       " 'I have to jiggle the plug to get it to line up right to get decent volume.',\n",
       " 'If you have several dozen or several hundred contacts then imagine the fun of sending each of them one by one.',\n",
       " 'If you are Razr owner...you must have this!',\n",
       " 'Needless to say I wasted my money.',\n",
       " 'What a waste of money and time!.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = dataset['text']\n",
    "reviews[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "print(word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1992, 25)\n",
      "So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "[  28   59    8   56  142   13   61    7  269    6   15   46   15    2\n",
      "  149  449    4   60  113    5 1429    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "padded_sequences = pad_sequences(sequences, padding='post', maxlen=25)\n",
    "\n",
    "print(padded_sequences.shape)\n",
    "print(reviews[0])\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  dataset['text'].to_list()\n",
    "labels =  dataset['sentiment'].to_list()\n",
    "\n",
    "training_size = int(len(sentences) * 0.8)\n",
    "\n",
    "training_sent = sentences[:training_size]\n",
    "testing_sent = sentences[training_size:]\n",
    "training_lbl = np.array(labels[:training_size])\n",
    "testing_lbl = np.array(labels[training_size:])\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim =16\n",
    "max_lenght = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "OOV_tok = '<OOV>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=OOV_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "seq = tokenizer.texts_to_sequences(training_sent)\n",
    "padded = pad_sequences(seq, maxlen=max_lenght, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(testing_sent)\n",
    "test_padded  = pad_sequences(test_seq, maxlen=max_lenght, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good case excellent value ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Good case Excellent value.\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([value, key] for (key, value) in word_index.items())\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded_sequences[1]))\n",
    "print(training_sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 100, 16)           16000     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 6)                 9606      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,613\n",
      "Trainable params: 25,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_lenght),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(6, activation = 'relu'),\n",
    "    keras.layers.Dense(1, activation = 'sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1593, 100), (1593,), (399, 100))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape, training_lbl.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "50/50 [==============================] - 1s 7ms/step - loss: 0.6934 - accuracy: 0.5286 - val_loss: 0.6927 - val_accuracy: 0.5764\n",
      "Epoch 2/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6873 - accuracy: 0.5480 - val_loss: 0.7031 - val_accuracy: 0.4110\n",
      "Epoch 3/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6750 - accuracy: 0.5430 - val_loss: 0.6915 - val_accuracy: 0.4561\n",
      "Epoch 4/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.6259 - val_loss: 0.6729 - val_accuracy: 0.5363\n",
      "Epoch 5/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5785 - accuracy: 0.8261 - val_loss: 0.6068 - val_accuracy: 0.7469\n",
      "Epoch 6/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.8908 - val_loss: 0.5367 - val_accuracy: 0.7519\n",
      "Epoch 7/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.9190 - val_loss: 0.5120 - val_accuracy: 0.7644\n",
      "Epoch 8/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.9335 - val_loss: 0.4769 - val_accuracy: 0.7719\n",
      "Epoch 9/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9517 - val_loss: 0.4906 - val_accuracy: 0.7569\n",
      "Epoch 10/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9611 - val_loss: 0.4695 - val_accuracy: 0.7519\n",
      "Epoch 11/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9674 - val_loss: 0.4430 - val_accuracy: 0.7845\n",
      "Epoch 12/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1341 - accuracy: 0.9730 - val_loss: 0.4607 - val_accuracy: 0.7719\n",
      "Epoch 13/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9780 - val_loss: 0.4451 - val_accuracy: 0.7744\n",
      "Epoch 14/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9824 - val_loss: 0.4444 - val_accuracy: 0.7820\n",
      "Epoch 15/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9837 - val_loss: 0.4616 - val_accuracy: 0.7820\n",
      "Epoch 16/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.9868 - val_loss: 0.4621 - val_accuracy: 0.7870\n",
      "Epoch 17/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9887 - val_loss: 0.4781 - val_accuracy: 0.7719\n",
      "Epoch 18/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9881 - val_loss: 0.4786 - val_accuracy: 0.7845\n",
      "Epoch 19/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9912 - val_loss: 0.4787 - val_accuracy: 0.7920\n",
      "Epoch 20/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0428 - accuracy: 0.9931 - val_loss: 0.4875 - val_accuracy: 0.7895\n",
      "Epoch 21/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.9925 - val_loss: 0.5065 - val_accuracy: 0.7845\n",
      "Epoch 22/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.9925 - val_loss: 0.5146 - val_accuracy: 0.7820\n",
      "Epoch 23/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9950 - val_loss: 0.5441 - val_accuracy: 0.7694\n",
      "Epoch 24/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9937 - val_loss: 0.5286 - val_accuracy: 0.7794\n",
      "Epoch 25/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9969 - val_loss: 0.5317 - val_accuracy: 0.7744\n",
      "Epoch 26/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9975 - val_loss: 0.5911 - val_accuracy: 0.7644\n",
      "Epoch 27/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9962 - val_loss: 0.6056 - val_accuracy: 0.7619\n",
      "Epoch 28/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.9969 - val_loss: 0.5533 - val_accuracy: 0.7845\n",
      "Epoch 29/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9969 - val_loss: 0.5653 - val_accuracy: 0.7769\n",
      "Epoch 30/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9975 - val_loss: 0.5845 - val_accuracy: 0.7769\n",
      "Epoch 31/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9975 - val_loss: 0.5989 - val_accuracy: 0.7694\n",
      "Epoch 32/32\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9975 - val_loss: 0.6032 - val_accuracy: 0.7669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19bfccbb790>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 32\n",
    "model.fit(padded, training_lbl, epochs=num_epochs, validation_data=(test_padded, testing_lbl))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c30f875c0d98aa4add518e958bf2a3afd5ef47919b69ca82c2c4929a9dd85b95"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.first_evn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
